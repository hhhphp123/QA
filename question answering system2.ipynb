{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- enviroment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from scipy import spatial\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim import models\n",
    "import string\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import urllib\n",
    "import sys\n",
    "import os\n",
    "import zipfile\n",
    "import tarfile\n",
    " \n",
    "import hashlib\n",
    "import re\n",
    "import itertools\n",
    "from sklearn import metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = \"/Users/alfredchen/Documents/GitHub/query-system/data/training.json\"\n",
    "dev_set = \"/Users/alfredchen/Documents/GitHub/query-system/data/devel.json\"\n",
    "test_set = \"/Users/alfredchen/Documents/GitHub/query-system/data/testing.json\"\n",
    "doc = \"/Users/alfredchen/Documents/GitHub/query-system/data/documents.json\"\n",
    "\n",
    "#save word embeddings\n",
    "embeddings = \"/Users/alfredchen/Documents/GitHub/query-system/models/mymodel-size\"\n",
    "\n",
    "# save qa log\n",
    "log_file = \"/Users/alfredchen/Documents/GitHub/query-system/data/log.txt\"\n",
    "answer_type = \"/Users/alfredchen/Documents/GitHub/query-system/data/answer_type_label.txt\"\n",
    "answer_file = \"/Users/alfredchen/Documents/GitHub/query-system/data/answer.txt\"\n",
    "ner_jar = \"/Users/alfredchen/Documents/GitHub/query-system/data/stanford-ner.jar\"\n",
    "ner_model = \"/Users/alfredchen/Documents/GitHub/query-system/data/english.muc.7class.distsim.crf.ser.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- preprocess corpus\n",
    "    - remove stopwords\n",
    "    - lemmatize\n",
    "    - lower case\n",
    "    - creat paragraph_index\n",
    "    - create index2paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_stopwords():\n",
    "        \n",
    "        words = list(nltk.corpus.stopwords.words())\n",
    "        \n",
    "        #some stopwords are helpful in targeting answer type, shall not be removed\n",
    "        words.remove('what')\n",
    "        words.remove('where')\n",
    "        words.remove('when')\n",
    "        words.remove('who')\n",
    "        words.remove('how')\n",
    "        words.remove('which')\n",
    "        # add more #\n",
    "\n",
    "        stopwords = {}\n",
    "        for word in words:\n",
    "            stopwords[word] = stopwords.get(word,0) + 1\n",
    "        return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word, 'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word, 'n')\n",
    "    return lemma\n",
    "\n",
    "def removePunc(string):\n",
    "    # define punctuation\n",
    "    punctuations = '''!()-[]{};:'\"\\,./?@#$%^&*_~'''\n",
    "    \n",
    "    # remove punctuation from the string\n",
    "    no_punct = \"\"\n",
    "    for char in string:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "\n",
    "    # display the unpunctuated string\n",
    "    #print(no_punct)\n",
    "    return no_punct\n",
    "\n",
    "\n",
    "stopwords = get_stopwords()        # get stopwords\n",
    "\n",
    "def preprocess_docs(corpus):\n",
    "        para2index = {}\n",
    "        index2para = {}\n",
    "        new_corpus = []\n",
    "        para_len = 0\n",
    "        para_count = 0\n",
    "        for _id, doc in enumerate(corpus):\n",
    "            new_doc = []\n",
    "            for _para,para in enumerate(doc['text']):\n",
    "                para2index[para] = (_id,_para)         # para index\n",
    "                index2para[(_id,_para)] = para         # doc_id, answer_para index\n",
    "                new_para = []\n",
    "                sents = para.split('.')\n",
    "                para_count += 1\n",
    "                for _sent,sent in enumerate(sents):\n",
    "                    new_sent=[]\n",
    "                    sent = removePunc(sent)\n",
    "                    #sent = '<s> ' + sent + ' <end>'  # padding\n",
    "                    words = sent.split(' ')\n",
    "                    s = ''.join(sent)\n",
    "                    if s == '<s> <end>':\n",
    "                        continue\n",
    "                    for word in words:\n",
    "                        word = word.strip(',')\n",
    "                        word = word.strip(',')\n",
    "                        word = word.strip('.')\n",
    "                        word = word.strip('?')\n",
    "                        word = word.strip('\\'s')\n",
    "                        word = word.strip('\\\"')\n",
    "                            #if not word.isalpha():\n",
    "                                #continue\n",
    "                        new_word = word.lower()\n",
    "                        new_word = lemmatize(new_word)\n",
    "                        if stopwords.get(new_word):\n",
    "                            continue\n",
    "                        if new_word == '':\n",
    "                            continue\n",
    "                        para_len += 1\n",
    "                        new_sent.append(new_word)                \n",
    "                    new_para.append(new_sent)\n",
    "                new_doc.append(new_para)\n",
    "            new_corpus.append(new_doc)\n",
    "        average_para_len = (para_len/para_count)\n",
    "        print(average_para_len)\n",
    "        return new_corpus, para2index, index2para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.1837880794702\n"
     ]
    }
   ],
   "source": [
    "docs = json.load(open(doc))\n",
    "corpus,para2index,index2para = preprocess_docs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get word embeddings based on pre processed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def w2v(corpora,size,iter):\n",
    "        docs = corpora\n",
    "        sentences = []\n",
    "        for doc in docs:\n",
    "            for sents in doc:\n",
    "                new_para = []\n",
    "                for sent in sents:\n",
    "                    new_para += sent\n",
    "                sentences.append(new_para)\n",
    "        model = Word2Vec(sentences, size=size, window=5,iter=iter,workers=4)\n",
    "        model.save(embeddings+str(size)+'-iter'+str(iter))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = w2v(corpus,160,150)   # train the model\n",
    "\n",
    "#load pre-trained model\n",
    "model = gensim.models.Word2Vec.load(embeddings+str(160)+'-iter'+str(150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freqs(corpus):\n",
    "    tfs = defaultdict(dict)\n",
    "    tfs_forward = defaultdict(dict)\n",
    "    doc_id = 0\n",
    "    for doc in corpus:\n",
    "        for para in doc:\n",
    "            for sent in para:\n",
    "                for term in sent:\n",
    "                    tfs[term][doc_id] = tfs[term].get(doc_id, 0) + 1 \n",
    "                    tfs_forward[doc_id][term] = tfs[doc_id].get(term, 0) + 1\n",
    "        doc_id += 1\n",
    "    return tfs,doc_id+1,tfs_forward\n",
    "\n",
    "\n",
    "tfs,total_docment,tfs_forward = term_freqs(corpus)\n",
    "\n",
    "def get_tfidf(tfs, total_docment,tfs_forward):\n",
    "    document_length = {}\n",
    "    for doc_id,doc_list in tfs_forward.items():\n",
    "        length = 0\n",
    "        for term, freq in doc_list.items():\n",
    "            length += freq ** 2\n",
    "        length = length **0.5\n",
    "        document_length[doc_id] = length\n",
    "    tfidf = defaultdict(dict)\n",
    "    for term, doc_list in tfs.items():\n",
    "        df = len(doc_list)\n",
    "        for doc_id, freq in doc_list.items(): \n",
    "            tfidf[term][doc_id] = (float(tfs[term][doc_id]) * log(total_docment / df)) / document_length.get(doc_id)\n",
    "    return tfidf\n",
    "\n",
    "tfidf = get_tfidf(tfs, total_docment,tfs_forward)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- transform sentence to feature vector (sentence embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    def get_weight(word,doc_id):\n",
    "        try:\n",
    "            weight = tfidf[word][doc_id]\n",
    "        except KeyError:\n",
    "            weight = 0.0001\n",
    "        return weight \n",
    "    \n",
    "    def sent2vec(sentence, model, size, index2word_set, doc_id):\n",
    "        \"\"\"transform word embeddings to sentence vector\n",
    "        param: sentence: sentence that want to be transformed\n",
    "        param: model: pre-trained word embeddings\n",
    "        param: size: feature vector dimension\n",
    "        param: index2word_set\n",
    "        return: transformed sentence vector\n",
    "        \"\"\"        \n",
    "        try:\n",
    "            words = sentence.split()  \n",
    "        except:\n",
    "            words = sentence\n",
    "            \n",
    "        words = [removePunc(x) for x in words]\n",
    "        feature_vec = np.zeros((size,), dtype='float32')\n",
    "        n_words = 0\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = lemmatize(word)\n",
    "            if stopwords.get(word):\n",
    "                continue\n",
    "            if word in index2word_set:\n",
    "                n_words += 1\n",
    "                if doc_id != 'disable':\n",
    "                    w = get_weight(word,doc_id)\n",
    "                else:\n",
    "                    w = 1\n",
    "                feature_vec = np.add(feature_vec, model.wv[word]*w)\n",
    "        if (n_words > 0):\n",
    "            feature_vec = np.divide(feature_vec, n_words)\n",
    "        return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18433142253254897\n"
     ]
    }
   ],
   "source": [
    "print(get_weight('achim',1))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- play with sentence vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6428887844085693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('king', 0.7751615643501282),\n",
       " ('balmoral', 0.748808741569519),\n",
       " ('duke', 0.7470282912254333),\n",
       " ('fief', 0.7448990941047668),\n",
       " ('royal', 0.7361407279968262),\n",
       " ('lambert', 0.7349075078964233),\n",
       " ('controversially', 0.7227569818496704),\n",
       " ('prince', 0.7206754088401794),\n",
       " ('frederick', 0.7180166244506836),\n",
       " ('elizabeth', 0.7173427939414978)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate similarity of two sentence\n",
    "\n",
    "s1_afv = sent2vec('A kilogram could be definined as having a Planck constant of what value?', model, 160, index2word_set,'disable')\n",
    "s2_afv = sent2vec('Possible new definitions include \"the mass of a body at rest whose equivalent energy equals the energy of photons whose frequencies sum to 7050135639273999999♠135639274×1042 Hz\", or simply \"the kilogram is defined so that the Planck constant equals 6966662606895999999♠6.62606896×10−34 J⋅s\".', model, 160, index2word_set,'disable')\n",
    "sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\n",
    "print(sim)\n",
    "model.wv.most_similar_cosmul(positive=['male', 'queen'], negative=['female'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get similarity ranking of query to paragraphs in a documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_sent(query, corpus):\n",
    "    sim2ipara = {}\n",
    "    index2sim = {}\n",
    "    for _id,doc in enumerate(corpus):\n",
    "        q = sent2vec(query, model, 160, index2word_set,_id)\n",
    "        for _para,para in enumerate(doc):\n",
    "            sentence = []\n",
    "            for sent in para:\n",
    "                sentence += sent\n",
    "            sentence = sent2vec(sentence, model, 160, index2word_set,'disable')\n",
    "            sim = 1 - spatial.distance.cosine(q, sentence)\n",
    "            index2sim[_id, _para] = sim\n",
    "            sim2ipara[sim] = (_id,_para)\n",
    "    return index2sim, sim2ipara\n",
    "    \n",
    "\n",
    "#index2sim, sim2ipara = sim_sent(\"what does the Planck constant refer to?\",corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(query, answer):\n",
    "    sim = 1 - spatial.distance.cosine(query, answer)\n",
    "    return sim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 5), (0, 1), (0, 16), (0, 13), (0, 23), (0, 11), (0, 17), (0, 2), (0, 10), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def getTopN(N,query):\n",
    "    top_document_id = Counter()\n",
    "    for token in word_tokenize(query):\n",
    "        token = token.lower()\n",
    "        token = lemmatize(token)\n",
    "        if not stopwords.get(token):\n",
    "            term_tfidf = tfidf[token]\n",
    "            for docid, weight in term_tfidf.items():\n",
    "                 top_document_id[docid] += weight\n",
    "    top_document_id = top_document_id.most_common(N)\n",
    "    top_document = []\n",
    "    for document_id,weight in top_document_id:\n",
    "        top_document.append(document_id)\n",
    "    sim2index = {}\n",
    "    q = sent2vec(query, model, 160, index2word_set,'disable')\n",
    "    for _id in top_document:\n",
    "        #q = sent2vec(query, model, 160, index2word_set,_id)\n",
    "        paras = [para for (doc,para) in index2para.keys() if doc == _id]\n",
    "        for _para in paras:\n",
    "            answer = index2para.get((_id,_para))\n",
    "            answer = sent2vec(answer, model, 160, index2word_set,'disable')\n",
    "            sim = sentence_similarity(q, answer)\n",
    "            sim2index[sim] = (_id,_para)\n",
    "    \n",
    "    similarity_ranks = sorted(sim2index.keys(),reverse=True)   #sims\n",
    "    rank = []\n",
    "    for i in range(N):\n",
    "        try:\n",
    "            rank.append(sim2index.get(similarity_ranks[i]))\n",
    "        except IndexError as e:\n",
    "            print(top_document_id)\n",
    "    logs_file = open(log_file,\"a\")\n",
    "    logs_file.write(str(rank)+'\\n')\n",
    "    logs_file.close()\n",
    "    return rank\n",
    "    \n",
    "        \n",
    "\n",
    "index = getTopN(10,\n",
    "                \"A kilogram could be definined as having a Planck constant of what value?\")    \n",
    "print(index)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get top n answer paragraph given quey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def getTopN(N,query):\n",
    "    index2sim, sim2ipara = sim_sent(query,corpus)\n",
    "    \n",
    "    similarity_ranks = sorted(index2sim.values(),reverse=True)   #sims\n",
    "    rank = []\n",
    "    for i in range(N):\n",
    "        rank.append(sim2ipara.get(similarity_ranks[i])) \n",
    "        \n",
    "    return rank\n",
    "    \n",
    "\n",
    "getTopN(10,\"Who developed the theory of inheritance known as pangenesis?\")\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test hit accuracy: return top 10 paragraphs, is given answer included?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_standard(dataset):\n",
    "    file = json.load(open(dataset))\n",
    "    gold2index={}\n",
    "    for line in file:\n",
    "        _id = line['docid']\n",
    "        try:\n",
    "            _para = line['answer_paragraph']\n",
    "        except KeyError:\n",
    "            _para = line['id']\n",
    "        gold2index[line['question']]=(_id,_para)\n",
    "    return gold2index\n",
    "\n",
    "def my_qa(n,dataset):\n",
    "    file = json.load(open(dataset))\n",
    "    qa2index={}\n",
    "    for i in range(n):\n",
    "        save = open(log_file,\"a\")\n",
    "        line = file[i]\n",
    "        query = line['question']\n",
    "        possible = getTopN(10,query)\n",
    "        qa2index[line['question']] = possible\n",
    "        record = line['question']+':'+str(possible)+'\\n'\n",
    "        save.write(record)\n",
    "        save.close()\n",
    "    return qa2index\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def acc(gold_standard,qas):\n",
    "    query = qas.keys()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i,q in enumerate(query):\n",
    "        gold = gold_standard.get(q)\n",
    "        if gold in qas.get(q):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    acc = correct/total\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#test on top 100 trainset\n",
    "gold_index = gold_standard(train_set)\n",
    "qa_index = my_qa(100,train_set)\n",
    "accu = acc(gold_index,qa_index)\n",
    "print(accu)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#test on entire devtset\n",
    "gold_index = gold_standard(dev_set)\n",
    "qa_index = my_qa(100,dev_set)\n",
    "acc = acc(gold_index,qa_index)\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- prepare relevance training data\n",
    "assign label 1 denote relevance, label0 denote irrelevance\n",
    "randomly choose incorrect query-answer pairs in trainset and labeled 0\n",
    "choose top N correct query-answer pairs in trainset and labeled 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('film', 0.6140520572662354),\n",
       " ('spielberg', 0.43539032340049744),\n",
       " ('console', 0.4009501338005066),\n",
       " ('sylvester', 0.3827870786190033),\n",
       " ('animate', 0.3810267746448517),\n",
       " ('tv', 0.37869757413864136),\n",
       " ('entertainment', 0.37179452180862427),\n",
       " ('stallone', 0.36797311902046204),\n",
       " ('tudio', 0.3666054904460907),\n",
       " ('television', 0.3636029362678528)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['movie'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_relevance_data(N):\n",
    "    file = json.load(open(train_set))\n",
    "    query = [x['question'] for x in file]\n",
    "    answer = [x['text'] for x in file]\n",
    "\n",
    "    pair = []\n",
    "    for i in range(N):\n",
    "        q = query[i]\n",
    "        a = answer[i]\n",
    "        pair.append(((q,a),1))\n",
    "        random_q = random_a = [i]\n",
    "    \n",
    "        while random_q[0] == i:\n",
    "            random_q = np.random.choice(N,1)\n",
    "        while random_a[0] == i or random_a[0] == random_q[0]:\n",
    "            random_a = np.random.choice(N,1)\n",
    "        q = query[random_q[0]]  \n",
    "        a = answer[random_a[0]]\n",
    "        pair.append(((q,a),0))\n",
    "    return pair\n",
    "\n",
    "trainset = prepare_relevance_data(40000)\n",
    "devset = prepare_relevance_data(3097)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_query(query,answer):\n",
    "    q = [x.lower().strip('?') for x in query.split()]\n",
    "    trans_form_query = []\n",
    "    for word in q:\n",
    "        if word == 'what' or \\\n",
    "        word == 'who' or \\\n",
    "        word == 'where' or \\\n",
    "        word == 'when' or \\\n",
    "        word == 'which'or \\\n",
    "        word == 'how'or \\\n",
    "        word == 'whose':\n",
    "            word = str(answer)\n",
    "        trans_form_query.append(word)\n",
    "    q = ' '.join(trans_form_query)\n",
    "    return q    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation extraction \n",
    "def extract_nouns(context):\n",
    "    constituents = []\n",
    "    blob = TextBlob(context)\n",
    "    constituents = blob.noun_phrases\n",
    "    return constituents\n",
    "\n",
    "def get_para_constituent(topk):\n",
    "    constituents = []\n",
    "    for index in topk:\n",
    "        context = index2para.get(index)      \n",
    "        constituents += extract_nouns(context)\n",
    "    return constituents\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#just a demo, need more pre-extracted potential answer sets\n",
    "\n",
    "import ast\n",
    "def prepare_train(N,dataset):\n",
    "    file = json.load(open(dataset))\n",
    "    f = open(log_file,\"r\")\n",
    "    corrects = []\n",
    "    wrongs = []\n",
    "    for i,tops in enumerate(f):\n",
    "        line = file[i]\n",
    "        query = line['question']\n",
    "        answer = line['text']\n",
    "        correct = transform_query(query,answer)\n",
    "        corrects.append(correct)\n",
    "        topk = ast.literal_eval(tops.strip())\n",
    "        constituents = get_para_constituent(topk)\n",
    "        for n in range(5):\n",
    "            cons = np.random.choice(constituents,1)\n",
    "            wrong = transform_query(query, cons[0])\n",
    "            wrongs.append(wrong)\n",
    "    return corrects, wrongs\n",
    "\n",
    "correct, wrong = prepare_train(4155,train_set)\n",
    "label = []\n",
    "\n",
    "for i in correct:\n",
    "    label.append(1)\n",
    "for i in wrong:\n",
    "    label.append(0)\n",
    "    \n",
    "print(\"done\")\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4741\n",
      "23705\n"
     ]
    }
   ],
   "source": [
    "print(len(correct))\n",
    "print(len(wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def prepare_feature_vector(correct,wrong):\n",
    "    matrix = []\n",
    "    for i in correct:\n",
    "        vector = sent2vec(i, model, 160, index2word_set,'disable')\n",
    "        matrix.append(vector)\n",
    "    for i in wrong:\n",
    "        vector = sent2vec(i, model, 160, index2word_set,'disable')\n",
    "        matrix.append(vector)\n",
    "    return matrix\n",
    "\n",
    "X = prepare_feature_vector(correct,wrong)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28446\n",
      "28446\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- binary task, long feature vector -> take SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def train(X,y):   \n",
    "    model = LogisticRegression()\n",
    "    best_model = model.fit(X, y)\n",
    "    return best_model\n",
    "            \n",
    "relevance_model = train(X,label)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(query,entity,classifier):\n",
    "    \n",
    "    q = transform_query(query,entity)\n",
    "    vector = sent2vec(q, model, 160, index2word_set,'disable')\n",
    "    preds = classifier.predict([vector][0:])\n",
    "    return preds\n",
    "    \n",
    "pred = pred(\"What example is given as another paired relationship of uncertainly related to standard deviation?\",\n",
    "            \"time vs. energy\",\n",
    "           relevance_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = []\n",
    "l = []\n",
    "for i in range(4741):\n",
    "    q.append(sent2vec(wrong[i], model, 160, index2word_set,'disable'))\n",
    "    l.append(0)\n",
    "for i in correct:\n",
    "    q.append(sent2vec(i, model, 160, index2word_set,'disable'))\n",
    "    l.append(1)\n",
    "    \n",
    "preds = relevance_model.predict(q[0:])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from sklearn.externals import joblib\n",
    "\n",
    "#save clf\n",
    "joblib.dump(relevance_model, '/Users/alfredchen/Documents/GitHub/query-system/models/relevance.pkl')\n",
    "\n",
    "#load clf\n",
    "#relevance_model = joblib.load('/Users/alfredchen/Documents/GitHub/query-system/models/relevance.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant(query, answer, classifier):\n",
    "    X = []\n",
    "    for i in range(len(query)):\n",
    "        q = transform_query(query[i],answer[i])\n",
    "          \n",
    "        x = sent2vec(q, model, 160, index2word_set,'disable')\n",
    "        #x = np.concatenate((q,a),axis=0)\n",
    "        X.append(x)\n",
    "    preds=relevance_model.predict(X[0:])\n",
    "    return preds\n",
    "\n",
    "#dev set\n",
    "\n",
    "query=[]\n",
    "answer = []\n",
    "labels = []\n",
    "for pair,label in devset:\n",
    "    query.append(pair[0])\n",
    "    answer.append(pair[1])\n",
    "    labels.append(label)\n",
    "    \n",
    "preds = relevant(query,answer,relevance_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.5046403712296984\n",
      "\n",
      "Average precision:\n",
      "0.6785870830765016\n",
      "\n",
      "Average recall:\n",
      "0.5046403712296984\n",
      "\n",
      "Average f1:\n",
      "0.3451919622241372\n",
      "\n",
      "Classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      1.00      0.67      4741\n",
      "          1       0.85      0.01      0.02      4741\n",
      "\n",
      "avg / total       0.68      0.50      0.35      9482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_metrices(y_test,y_pred):\n",
    "    print(\"Accuracy:\")\n",
    "    print(metrics.accuracy_score(y_test,y_pred))\n",
    "\n",
    "    print(\"\\nAverage precision:\")\n",
    "    print(metrics.precision_score(y_test,y_pred,average='macro'))\n",
    "\n",
    "    print(\"\\nAverage recall:\")\n",
    "    print(metrics.recall_score(y_test,y_pred,average='macro'))\n",
    "    \n",
    "    print(\"\\nAverage f1:\")\n",
    "    print(metrics.f1_score(y_test,y_pred,average='macro'))\n",
    "\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    \n",
    "print_metrices(l, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kilogram', 'define', 'what', 'value']\n",
      "['kilogram', 'define', 'value']\n",
      "['kilogram', 'define', 'what', 'value']\n",
      "defaultdict(<class 'dict'>, {'kilogram': {'define': 1, 'value': 1}, 'define': {'kilogram': 1, 'value': 1}, 'value': {'kilogram': 1, 'define': 1}})\n"
     ]
    }
   ],
   "source": [
    "def keywords_extraction(query):\n",
    "    query = query.split(' ')\n",
    "    query = [removePunc(x) for x in query]\n",
    "    query = [lemmatize(x.lower()) for x in query]\n",
    "    q = []\n",
    "    for w in query:\n",
    "        if not stopwords.get(w) and w != '':\n",
    "            q.append(w)\n",
    "    print(q)\n",
    "    tagged = nltk.pos_tag(q,tagset=\"universal\")\n",
    "    return [p[0] for p in tagged if p[1] in [\"ADJ\",\"NOUN\",\"VERB\",\"NUM\"]]\n",
    "\n",
    "\n",
    "def relation_extraction(query):\n",
    "    words = keywords_extraction(query)\n",
    "    if words == []:\n",
    "        return\n",
    "    relations = defaultdict(dict)\n",
    "    for w1 in words:\n",
    "        for w2 in words:\n",
    "            if w1 == w2:\n",
    "                continue\n",
    "            else:\n",
    "                relations[w1][w2] = relations[w1].get(w2, 0) + 1\n",
    "    return relations\n",
    "\n",
    "pos = keywords_extraction(\"a kilogram can be defined as what value\")\n",
    "print(pos)\n",
    "relations = relation_extraction(\"a kilogram can be defined as what value?\")\n",
    "print(relations)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def answer_type(query):\n",
    "    query = query.split()\n",
    "    query = [removePunc(x) for x in query]\n",
    "    query = [lemmatize(x.lower()) for x in query]\n",
    "    for word in query:\n",
    "        if word == 'what':\n",
    "            return 'O'\n",
    "        elif word == 'who':\n",
    "            return 'PERSON'\n",
    "        elif word == 'where':\n",
    "            return 'LOCATION'\n",
    "        elif word == 'when':\n",
    "            return 'DATE'\n",
    "        elif word == 'which':\n",
    "            return 'O'\n",
    "    return 'O'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gensim/models/doc2vec.py:362: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-2d2c6708edbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrefined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mFinalIndexDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"what is the capital of china\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-155-2d2c6708edbc>\u001b[0m in \u001b[0;36mFinalIndexDoc2Vec\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     24\u001b[0m                                           \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_window\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                           \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                                           negative = 5, iter = max_iter)\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mnew_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, docvecs, docvecs_mapfile, comment, trim_rule, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the documents argument. Try an iterator.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             self.train(\n\u001b[1;32m    405\u001b[0m                 \u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, documents, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m         \"\"\"\n\u001b[1;32m    728\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0;32m--> 729\u001b[0;31m             documents, self.docvecs, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         report_values = self.vocabulary.prepare_vocab(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, documents, docvecs, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdocument_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m                     logger.warning(\n\u001b[1;32m    811\u001b[0m                         \u001b[0;34m\"Each 'words' should be a list of words (usually unicode strings). \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "def topk_sentence(para,keywords):\n",
    "    sentences = para.split('.')\n",
    "    for sent in sentences:\n",
    "        overlap = 0\n",
    "        words = keywords_extraction(sent)\n",
    "        for w in keywords:\n",
    "            if w in words:\n",
    "                overlap += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
