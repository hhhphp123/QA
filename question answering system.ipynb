{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- enviroment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from scipy import spatial\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = \"/Users/alfredchen/Documents/GitHub/query-system/data/training.json\"\n",
    "dev_set = \"/Users/alfredchen/Documents/GitHub/query-system/data/devel.json\"\n",
    "test_set = \"/Users/alfredchen/Documents/GitHub/query-system/data/testing.json\"\n",
    "doc = \"/Users/alfredchen/Documents/GitHub/query-system/data/documents.json\"\n",
    "\n",
    "#save word embeddings\n",
    "embeddings = \"/Users/alfredchen/Documents/GitHub/query-system/models/mymodel-size\"\n",
    "\n",
    "# save qa log\n",
    "log = \"/Users/alfredchen/Documents/GitHub/query-system/data/log.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- preprocess corpus\n",
    "    - remove stopwords\n",
    "    - lemmatize\n",
    "    - lower case\n",
    "    - creat paragraph_index\n",
    "    - create index2paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_stopwords():\n",
    "        \n",
    "        words = list(nltk.corpus.stopwords.words())\n",
    "        \n",
    "        #some stopwords are helpful in targeting answer type, shall not be removed\n",
    "        words.remove('what')\n",
    "        words.remove('where')\n",
    "        words.remove('when')\n",
    "        words.remove('who')\n",
    "        words.remove('how')\n",
    "        words.remove('which')\n",
    "        # add more #\n",
    "\n",
    "        stopwords = {}\n",
    "        for word in words:\n",
    "            stopwords[word] = stopwords.get(word,0) + 1\n",
    "        return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word, 'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word, 'n')\n",
    "    return lemma\n",
    "\n",
    "\n",
    "stopwords = get_stopwords()        # get stopwords\n",
    "\n",
    "def preprocess_docs(corpus):\n",
    "        para2index = {}\n",
    "        index2para = {}\n",
    "        new_corpus = []\n",
    "        for _id, doc in enumerate(corpus):\n",
    "            new_doc = []\n",
    "            for _para,para in enumerate(doc['text']):\n",
    "                para2index[para] = (_id,_para)         # para index\n",
    "                index2para[(_id,_para)] = para         # doc_id, answer_para index\n",
    "                new_para = []\n",
    "                sents = para.split('.')\n",
    "                for _sent,sent in enumerate(sents):\n",
    "                    new_sent=[]\n",
    "                    sent = '<s> ' + sent + ' <end>'  # padding\n",
    "                    words = sent.split(' ')\n",
    "                    s = ''.join(sent)\n",
    "                    if s == '<s>  <end>':\n",
    "                        continue\n",
    "                    for word in words:\n",
    "                        word = word.strip(',')\n",
    "                            #if not word.isalpha():\n",
    "                                #continue\n",
    "                        new_word = word.lower()\n",
    "                        if stopwords.get(new_word):\n",
    "                            continue\n",
    "                        new_word = lemmatize(new_word)\n",
    "                        new_sent.append(new_word)                \n",
    "                    new_para.append(new_sent)\n",
    "                new_doc.append(new_para)\n",
    "            new_corpus.append(new_doc)\n",
    "        return new_corpus, para2index, index2para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = json.load(open(doc))\n",
    "corpus,para2index,index2para = preprocess_docs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get word embeddings based on pre processed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def w2v(corpora,size,iter):\n",
    "        docs = corpora\n",
    "        sentences = []\n",
    "        for doc in docs:\n",
    "            for sents in doc:\n",
    "                new_para = []\n",
    "                for sent in sents:\n",
    "                    new_para += sent\n",
    "                sentences.append(new_para)\n",
    "        model = Word2Vec(sentences, size=size, iter=iter)\n",
    "        model.save(embeddings+str(size)+'-iter'+str(iter))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = w2v(corpus,100,200)   # train the model\n",
    "\n",
    "#load pre-trained model\n",
    "#model = gensim.models.Word2Vec.load(embeddings+str(300)+'-iter'+str(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- transform sentence to feature vector (sentence embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def sent2vec(sentence, model, size, index2word_set):\n",
    "        \"\"\"transform word embeddings to sentence vector\n",
    "        param: sentence: sentence that want to be transformed\n",
    "        param: model: pre-trained word embeddings\n",
    "        param: size: feature vector dimension\n",
    "        param: index2word_set\n",
    "        return: transformed sentence vector\n",
    "        \"\"\"\n",
    "        try:\n",
    "            words = sentence.split()\n",
    "        except:\n",
    "            words = sentence\n",
    "        feature_vec = np.zeros((size,), dtype='float32')\n",
    "        n_words = 0\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if stopwords.get(word):\n",
    "                continue\n",
    "            if word in index2word_set:\n",
    "                n_words += 1\n",
    "                feature_vec = np.add(feature_vec, model.wv[word])\n",
    "        if (n_words > 0):\n",
    "            feature_vec = np.divide(feature_vec, n_words)\n",
    "        return feature_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- play with sentence vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.785611629486084\n"
     ]
    }
   ],
   "source": [
    "# calculate similarity of two sentence\n",
    "\n",
    "s1_afv = sent2vec('what is the capital of china', model, 100, index2word_set)\n",
    "s2_afv = sent2vec('the capital of china is beijing', model, 100, index2word_set)\n",
    "sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(index2para.get((4,1)))     #get the first paragraph in 4th document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get similarity ranking of query to paragraphs in a documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_sent(query, corpus):\n",
    "    sim2ipara = {}\n",
    "    index2sim = {}\n",
    "    query = sent2vec(query, model, 100, index2word_set)\n",
    "    for _id,doc in enumerate(corpus):\n",
    "        for _para,para in enumerate(doc):\n",
    "            sentence = []\n",
    "            for sent in para:\n",
    "                sentence += sent\n",
    "            sentence = sent2vec(sentence, model, 100, index2word_set)\n",
    "            sim = 1 - spatial.distance.cosine(query, sentence)\n",
    "            index2sim[_id, _para] = sim\n",
    "            sim2ipara[sim] = (_id,_para)\n",
    "    return index2sim, sim2ipara\n",
    "    \n",
    "\n",
    "#index2sim, sim2ipara = sim_sent(\"what does the Planck constant refer to?\",corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get top n answer paragraph given quey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(358, 4),\n",
       " (439, 0),\n",
       " (57, 1),\n",
       " (282, 4),\n",
       " (191, 32),\n",
       " (345, 1),\n",
       " (127, 0),\n",
       " (187, 23),\n",
       " (385, 1),\n",
       " (57, 19)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getTopN(N,query):\n",
    "    index2sim, sim2ipara = sim_sent(query,corpus)\n",
    "    similarity_ranks = sorted(index2sim.values(),reverse=True)   #sims\n",
    "    rank = []\n",
    "    for i in range(N):\n",
    "        rank.append(sim2ipara.get(similarity_ranks[i])) \n",
    "    return rank\n",
    "\n",
    "getTopN(10,\"what does plank constant refer do\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In 1901, the Federation of Australia was the process by which the six separate British self-governing colonies of New South Wales, Queensland, South Australia, Tasmania, Victoria and Western Australia formed one nation. They kept the systems of government that they had developed as separate colonies but also would have a federal government that was responsible for matters concerning the whole nation. When the Constitution of Australia came into force, the colonies collectively became states of the Commonwealth of Australia.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#play: what is the capital of Australia\n",
    "\n",
    "index2para.get((358,4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test hit accuracy: return top 10 paragraphs, is given answer included?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_standard(dataset):\n",
    "    file = json.load(open(dataset))\n",
    "    gold2index={}\n",
    "    for line in file:\n",
    "        _id = line['docid']\n",
    "        try:\n",
    "            _para = line['answer_paragraph']\n",
    "        except KeyError:\n",
    "            _para = line['id']\n",
    "        gold2index[line['question']]=(_id,_para)\n",
    "    return gold2index\n",
    "\n",
    "def my_qa(n,dataset):\n",
    "    file = json.load(open(dataset))\n",
    "    qa2index={}\n",
    "    for i in range(n):\n",
    "        save = open(log,\"a\")\n",
    "        line = file[i]\n",
    "        query = line['question']\n",
    "        possible = getTopN(20,query)\n",
    "        qa2index[line['question']] = possible\n",
    "        record = line['question']+':'+str(possible)+'\\n'\n",
    "        save.write(record)\n",
    "        save.close()\n",
    "    return qa2index\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(gold_standard,qas):\n",
    "    query = qas.keys()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i,q in enumerate(query):\n",
    "        gold = gold_standard.get(q)\n",
    "        if gold in qas.get(q):\n",
    "            correct += 1\n",
    "        total = i\n",
    "    acc = correct/total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "#test on top 100 trainset\n",
    "gold_index = gold_standard(train_set)\n",
    "qa_index = my_qa(10,train_set)\n",
    "accu = acc(gold_index,qa_index)\n",
    "print(accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on entire devtset\n",
    "gold_index = gold_standard(dev_set)\n",
    "qa_index = my_qa(63,dev_set)\n",
    "acc = acc(gold_index,qa_index)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
